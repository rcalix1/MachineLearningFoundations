{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import torch\n",
    "import imageio\n",
    "import torchvision\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "from torchvision import transforms\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "import torchvision.transforms as T\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## CNNs for CIFAR10\n",
    "\n",
    "* https://rcalix1.github.io/DeepLearningAlgorithms/SecondEdition/chapter6_CNNs/index.html\n",
    "\n",
    "## Load and save torch model checkpoints\n",
    "\n",
    "* https://pytorch.org/tutorials/beginner/saving_loading_models.html\n",
    "\n",
    "## Data\n",
    "\n",
    "* Data: https://github.com/YoongiKim/CIFAR-10-images/tree/master\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import precision_score, recall_score, accuracy_score, f1_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## !pip install torchvision\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "torch.cuda.is_available()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "torch.cuda.device_count()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "torch.cuda.current_device()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'NVIDIA A30'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "torch.cuda.get_device_name(0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Assign device\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "torch_device = torch.device(\"cpu\")\n",
    "\n",
    "if torch.cuda.is_available(): \n",
    "    torch_device = torch.device(\"cuda\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "torch_device\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## CIFAR10 DATA\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "raw_data_train = '/home/rcalix/Desktop/CIFAR-10-images-master/train/'\n",
    "\n",
    "raw_data_test  = '/home/rcalix/Desktop/CIFAR-10-images-master/test/'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## PATH to checkpoint\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "PATH = \"/scratch/scholar/rcalix/CNN_model_CIFAR10\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Train data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dataset_train = []\n",
    "labels_train  = []\n",
    "targets_train = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for folder in os.listdir( raw_data_train ):\n",
    "    ## print(folder)\n",
    "    for image in os.listdir( os.path.join(raw_data_train, folder) ):\n",
    "        if folder not in labels_train:\n",
    "            labels_train.append( folder )\n",
    "        targets_train.append(  labels_train.index(folder)  )\n",
    "        img_arr = imageio.imread(  os.path.join(raw_data_train, folder, image), pilmode=\"RGB\"  )\n",
    "        \n",
    "        img = torch.from_numpy( img_arr ).permute( 2, 0, 1 ).float()\n",
    "        \n",
    "        img /= 255\n",
    "        dataset_train.append(img)\n",
    "        \n",
    "      \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "len(labels_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50000"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "len( targets_train )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 32, 32])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "dataset_train[3].shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data_train    = torch.stack( dataset_train )\n",
    "targets_train = torch.Tensor(  targets_train  ).type(   torch.LongTensor   )\n",
    "\n",
    "torch.save(   (data_train, targets_train, labels_train), \"InClass_CIFAR10_data\"     )\n",
    "\n",
    "## data1, targets1, labels1 = torch.load(\"InClass_CIFAR10_data\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([50000, 3, 32, 32])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "data_train.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([50000])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "targets_train.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 32, 32])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "data_train[4].shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
       "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
       "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
       "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
       "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
       "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
       "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
       "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
       "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
       "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
       "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
       "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
       "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
       "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
       "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
       "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
       "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
       "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
       "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
       "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
       "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
       "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
       "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
       "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
       "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
       "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
       "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
       "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
       "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
       "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
       "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
       "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
       "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
       "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
       "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
       "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
       "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
       "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
       "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
       "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
       "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
       "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "targets_train[24000:25000]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Print images\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.0000, 0.0000, 0.0000,  ..., 0.0157, 0.0235, 0.0196],\n",
       "         [0.0000, 0.0039, 0.0078,  ..., 0.0196, 0.0118, 0.0118],\n",
       "         [0.0157, 0.0039, 0.0000,  ..., 0.0078, 0.0039, 0.0196],\n",
       "         ...,\n",
       "         [0.0000, 0.0000, 0.0000,  ..., 0.0353, 0.0314, 0.0353],\n",
       "         [0.0353, 0.0275, 0.0000,  ..., 0.0118, 0.0157, 0.0157],\n",
       "         [0.0000, 0.0235, 0.0000,  ..., 0.0157, 0.0235, 0.0275]],\n",
       "\n",
       "        [[0.0078, 0.0078, 0.0157,  ..., 0.0000, 0.0000, 0.0000],\n",
       "         [0.0118, 0.0157, 0.0196,  ..., 0.0118, 0.0000, 0.0000],\n",
       "         [0.0157, 0.0039, 0.0039,  ..., 0.0078, 0.0039, 0.0196],\n",
       "         ...,\n",
       "         [0.0000, 0.0000, 0.0000,  ..., 0.0196, 0.0157, 0.0118],\n",
       "         [0.0353, 0.0275, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0235, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],\n",
       "\n",
       "        [[0.0000, 0.0000, 0.0000,  ..., 0.0039, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000,  ..., 0.0157, 0.0000, 0.0000],\n",
       "         [0.0078, 0.0000, 0.0000,  ..., 0.0157, 0.0039, 0.0196],\n",
       "         ...,\n",
       "         [0.0078, 0.0078, 0.0078,  ..., 0.0235, 0.0196, 0.0196],\n",
       "         [0.0353, 0.0275, 0.0078,  ..., 0.0157, 0.0157, 0.0157],\n",
       "         [0.0000, 0.0235, 0.0078,  ..., 0.0196, 0.0235, 0.0275]]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "img_tr = data_train[46000]\n",
    "img_tr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "transform = T.ToPILImage()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "img = transform(  img_tr  )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<PIL.Image.Image image mode=RGB size=32x32 at 0x2AEBFDDE4050>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "img\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Class balance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "y_train_np = targets_train.numpy() \n",
    "y_train_np.shape\n",
    "\n",
    "the_set = np.unique(  y_train_np  )\n",
    "the_set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "_ = plt.hist( targets_train.numpy() , bins=\"auto\" )\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Test Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dataset_test = []\n",
    "labels_test = []\n",
    "targets_test = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for folder in os.listdir( raw_data_test ):\n",
    "    ## print(folder)\n",
    "    for image in os.listdir( os.path.join(raw_data_test, folder) ):\n",
    "        if folder not in labels_test:\n",
    "            labels_test.append( folder )\n",
    "        targets_test.append(  labels_test.index(folder)  )\n",
    "        img_arr = imageio.imread(  os.path.join(raw_data_test, folder, image), pilmode=\"RGB\"  )\n",
    "        \n",
    "        img = torch.from_numpy( img_arr ).permute( 2, 0, 1 ).float()\n",
    "        \n",
    "        img /= 255\n",
    "        dataset_test.append(img)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data_test   = torch.stack( dataset_test )\n",
    "targets_test = torch.Tensor(  targets_test  ).type(   torch.LongTensor   )\n",
    "\n",
    "torch.save(   (data_test, targets_test, labels_test), \"InClass_CIFAR10_data_test\"     )\n",
    "\n",
    "## data1, targets1, labels1 = torch.load(\"InClass_CIFAR10_data\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10000, 3, 32, 32])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "data_test.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10000])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "targets_test.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "_ = plt.hist( targets_test.numpy() , bins=\"auto\" )\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X_train = data_train  \n",
    "y_train = targets_train\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X_test = data_test  \n",
    "y_test = targets_test \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Change to float 32\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X_train = X_train.numpy()\n",
    "X_test  = X_test.numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X_train = X_train.astype(  np.float32  )\n",
    "X_test  = X_test.astype(   np.float32  )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X_train = torch.from_numpy(X_train )\n",
    "X_test = torch.from_numpy( X_test  )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Print shapes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50000"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "X_train.shape[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "int"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "y_train[30000].item()\n",
    "type(y_train[30000].item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "y_train[30000].item()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.4510, 0.4510, 0.4431,  ..., 0.2549, 0.3333, 0.3529],\n",
       "         [0.4863, 0.4980, 0.4941,  ..., 0.2863, 0.3608, 0.3608],\n",
       "         [0.4353, 0.4588, 0.4706,  ..., 0.3098, 0.3608, 0.3255],\n",
       "         ...,\n",
       "         [0.4510, 0.4627, 0.4549,  ..., 0.4706, 0.4510, 0.4353],\n",
       "         [0.4392, 0.4549, 0.4510,  ..., 0.4667, 0.4549, 0.4431],\n",
       "         [0.4157, 0.4471, 0.4588,  ..., 0.4431, 0.4392, 0.4353]],\n",
       "\n",
       "        [[0.5294, 0.5255, 0.5176,  ..., 0.2980, 0.3843, 0.4039],\n",
       "         [0.5569, 0.5725, 0.5686,  ..., 0.3294, 0.4118, 0.4118],\n",
       "         [0.5059, 0.5333, 0.5333,  ..., 0.3529, 0.4039, 0.3765],\n",
       "         ...,\n",
       "         [0.4627, 0.4745, 0.4745,  ..., 0.4824, 0.4784, 0.4627],\n",
       "         [0.4588, 0.4745, 0.4706,  ..., 0.4784, 0.4784, 0.4784],\n",
       "         [0.4353, 0.4667, 0.4784,  ..., 0.4667, 0.4627, 0.4667]],\n",
       "\n",
       "        [[0.5255, 0.5176, 0.5098,  ..., 0.2314, 0.3059, 0.3255],\n",
       "         [0.5569, 0.5647, 0.5608,  ..., 0.2627, 0.3412, 0.3412],\n",
       "         [0.5059, 0.5255, 0.5294,  ..., 0.2863, 0.3373, 0.3059],\n",
       "         ...,\n",
       "         [0.3804, 0.3922, 0.3961,  ..., 0.4392, 0.4157, 0.3922],\n",
       "         [0.3725, 0.3882, 0.3922,  ..., 0.4353, 0.4235, 0.4118],\n",
       "         [0.3490, 0.3882, 0.4000,  ..., 0.4275, 0.4078, 0.4078]]])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    " X_train[78]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    " CIFAR_train_list = [  ( X_train[i],  y_train[i].item() )  for i in range( X_train.shape[0]   )  ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    " CIFAR_test_list = [  ( X_test[i],  y_test[i].item() )  for i in range( X_test.shape[0]   )  ]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## DataLoader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "batch_size = 16\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_dl = torch.utils.data.DataLoader( CIFAR_train_list, batch_size=batch_size, shuffle=True  )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "test_dl = torch.utils.data.DataLoader( CIFAR_test_list, batch_size=10000, shuffle=True  )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Architectures\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3072"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "32*32*3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class DL_3h_net(nn.Module):\n",
    "    ## init the class\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.linear1 = nn.Linear( 32*32*3 , 200)\n",
    "        self.act1    = nn.ReLU()\n",
    "        \n",
    "        self.linear2 = nn.Linear(200 , 100)\n",
    "        self.act2   = nn.ReLU()\n",
    "        \n",
    "        self.linear3 = nn.Linear( 100 ,50)\n",
    "        self.act3    = nn.ReLU()\n",
    "        \n",
    "        self.linear4 = nn.Linear(50 , 10)\n",
    "        self.act4    = nn.Softmax(dim=1)\n",
    "        \n",
    "        ## self.norm    = nn.LayerNorm()\n",
    "        \n",
    "    ## perform inference\n",
    "    def forward(self, x):\n",
    "        \n",
    "        x            = self.linear1(x)\n",
    "        x            = self.act1(x)\n",
    "        x            = self.linear2(x)\n",
    "        x            = self.act2(x)\n",
    "        x            = self.linear3(x)\n",
    "        x            = self.act3(x)\n",
    "      \n",
    "        x            = self.linear4(x)\n",
    "        y_pred       = self.act4(x)\n",
    "        \n",
    "        return y_pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class CNN_net(nn.Module):\n",
    "    ## init the class\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.model = nn.Sequential(\n",
    "            \n",
    "            ## conv layer 1\n",
    "            nn.Conv2d(3, 16, kernel_size=5, stride=1, padding=1  ),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.MaxPool2d(2, 2), \n",
    "            nn.Dropout(0.2),\n",
    "            \n",
    "            ## conv layer 2\n",
    "            nn.Conv2d(16, 32, kernel_size=5, stride=1, padding=1  ),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.MaxPool2d(2, 2), \n",
    "            nn.Dropout(0.2),\n",
    "            \n",
    "            ## conv layer 2\n",
    "            nn.Conv2d(32, 32, kernel_size=5, stride=1, padding=1  ),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.MaxPool2d(2, 2), \n",
    "            nn.Dropout(0.2),\n",
    "              \n",
    "            ## FeedForward\n",
    "            \n",
    "            nn.Flatten(),\n",
    "            nn.Linear( 128 , 512),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            \n",
    "            nn.Linear(512, 10),\n",
    "            nn.Softmax(dim=1)        ## nn.LogSoftmax()\n",
    "            \n",
    "        \n",
    "        )\n",
    "        \n",
    "    ## perform inference\n",
    "    def forward(self, x):\n",
    "        \n",
    "        y_pred = self.model( x  )\n",
    "       \n",
    "        \n",
    "        return y_pred\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class MLP_net(nn.Module):\n",
    "    ## init the class\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.linear1 = nn.Linear( 32*32*3 ,20)\n",
    "        self.act1    = nn.LeakyReLU()\n",
    "        self.linear2 = nn.Linear(20 , 10)\n",
    "        self.act2    = nn.Softmax(dim=1)\n",
    "        \n",
    "        self.norm    = nn.LayerNorm()\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        \n",
    "        \n",
    "    ## perform inference\n",
    "    def forward(self, x):\n",
    "        \n",
    "        x            = self.linear1(x)\n",
    "        x            = self.act1(x)\n",
    "        x            = self.norm(x)\n",
    "        x            = self.dropout(x)\n",
    "        \n",
    "        x            = self.linear2(x)\n",
    "        y_pred       = self.act2(x)\n",
    "        \n",
    "        return y_pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def training_loop( N_Epochs, model, loss_fn, opt ):\n",
    "    for epoch in range(N_Epochs):\n",
    "        for xb, yb in train_dl:\n",
    "            \n",
    "            ## xb = xb.view(  (16, -1 ) )\n",
    "            \n",
    "            xb = xb.to( torch_device )\n",
    "            yb = yb.to( torch_device )\n",
    "            \n",
    "            y_pred = model(xb)\n",
    "            \n",
    "            loss = loss_fn(y_pred, yb)\n",
    "            \n",
    "            opt.zero_grad()\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "        \n",
    "        if epoch % 5 == 0:\n",
    "            print(epoch, \"loss=\", loss)\n",
    "            new_PATH = PATH + str(epoch)\n",
    "            print( new_PATH )\n",
    "            torch.save(model, new_PATH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## torch.save(model, PATH)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Core functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "N_Epochs      = 60\n",
    "learning_rate = 0.001\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## model = MLP_net()\n",
    "## model =  DL_3h_net()\n",
    "\n",
    "model = CNN_net()\n",
    "\n",
    "model.to( torch_device )\n",
    "\n",
    "opt = torch.optim.Adam(  model.parameters(), lr=learning_rate  )\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 loss= tensor(2.0991, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "/scratch/scholar/rcalix/CNN_model_CIFAR100\n",
      "5 loss= tensor(2.2257, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "/scratch/scholar/rcalix/CNN_model_CIFAR105\n",
      "10 loss= tensor(1.9590, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "/scratch/scholar/rcalix/CNN_model_CIFAR1010\n",
      "15 loss= tensor(1.9612, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "/scratch/scholar/rcalix/CNN_model_CIFAR1015\n",
      "20 loss= tensor(2.0249, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "/scratch/scholar/rcalix/CNN_model_CIFAR1020\n",
      "25 loss= tensor(2.0859, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "/scratch/scholar/rcalix/CNN_model_CIFAR1025\n",
      "30 loss= tensor(2.1487, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "/scratch/scholar/rcalix/CNN_model_CIFAR1030\n",
      "35 loss= tensor(1.8987, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "/scratch/scholar/rcalix/CNN_model_CIFAR1035\n",
      "40 loss= tensor(1.9612, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "/scratch/scholar/rcalix/CNN_model_CIFAR1040\n",
      "45 loss= tensor(2.0237, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "/scratch/scholar/rcalix/CNN_model_CIFAR1045\n",
      "50 loss= tensor(1.7742, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "/scratch/scholar/rcalix/CNN_model_CIFAR1050\n",
      "55 loss= tensor(2.0237, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "/scratch/scholar/rcalix/CNN_model_CIFAR1055\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "training_loop( N_Epochs, model, loss_fn, opt )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def print_metrics_function(y_test, y_pred):\n",
    "    print('Accuracy: %.2f' % accuracy_score(y_test, y_pred))\n",
    "    confmat = confusion_matrix(y_true=y_test, y_pred=y_pred)\n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(confmat)\n",
    "    print('Precision: %.3f' % precision_score(y_true=y_test, y_pred=y_pred, average='weighted'))\n",
    "    print('Recall: %.3f' % recall_score(y_true=y_test, y_pred=y_pred, average='weighted'))\n",
    "    print('F1-measure: %.3f' % f1_score(y_true=y_test, y_pred=y_pred, average='weighted'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.16\n",
      "Confusion Matrix:\n",
      "[[347  84  19  22  38  37  38  89  34 292]\n",
      " [ 24   5  40 121 674  69   5  17  10  35]\n",
      " [  2  13   8 719  59 168  10   5   4  12]\n",
      " [111 553  24  35  24  65  14  39  35 100]\n",
      " [ 50 178  32  50  37  16  58 133 307 139]\n",
      " [ 19  33   4 212  86 586  18   8   4  30]\n",
      " [ 94  59  75  45  40  27  71 319 110 160]\n",
      " [ 38  44   8  47  24  49 451  56 107 176]\n",
      " [ 21  23 315 120 293  77  22  76  16  37]\n",
      " [125  74  31  57  45  56  56  74  34 448]]\n",
      "Precision: 0.150\n",
      "Recall: 0.161\n",
      "F1-measure: 0.153\n"
     ]
    }
   ],
   "source": [
    "\n",
    "with torch.no_grad():\n",
    "    for x_real, y_real in test_dl:\n",
    "        \n",
    "        batch_size = x_real.shape[0]\n",
    "        \n",
    "        ## x_real = x_real.view(  (batch_size, -1 ) )\n",
    "        \n",
    "        x_real = x_real.to( torch_device )\n",
    "        \n",
    "        y_pred = model(  x_real  )\n",
    "        \n",
    "        vals, indeces = torch.max( y_pred, dim=1  )\n",
    "        preds = indeces\n",
    "        print_metrics_function( y_real, preds.cpu() )\n",
    "   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## From checkpoint\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Model class must be defined somewhere\n",
    "model2 = torch.load('/scratch/scholar/rcalix/CNN_model_CIFAR1010')\n",
    "## model.eval()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with torch.no_grad():\n",
    "    for x_real, y_real in test_dl:\n",
    "        \n",
    "        batch_size = x_real.shape[0]\n",
    "        \n",
    "        ## x_real = x_real.view(  (batch_size, -1 ) )\n",
    "        \n",
    "        y_pred = model2(  x_real  )\n",
    "        \n",
    "        vals, indeces = torch.max( y_pred, dim=1  )\n",
    "        preds = indeces\n",
    "        print_metrics_function(y_real, preds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "rc = data_train[4].view((-1))\n",
    "rc.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "rc = torch.unsqueeze(rc, dim=0)\n",
    "rc.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## example_label = model( rc )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## example_label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Figure out the dimensions of CNN\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "N_batches_rc = 16\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "\n",
    "model_rc = nn.Sequential(\n",
    "            \n",
    "            ## conv layer 1\n",
    "            nn.Conv2d(3, 16, kernel_size=5, stride=1, padding=1  ),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2), \n",
    "            \n",
    "            ## conv layer 2\n",
    "            nn.Conv2d(16, 32, kernel_size=5, stride=1, padding=1  ),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2), \n",
    "              \n",
    "            ## FeedForward\n",
    "            \n",
    "            nn.Flatten()\n",
    "            \n",
    "            \n",
    "        \n",
    "        )\n",
    "        \n",
    "'''\n",
    "\n",
    "\n",
    "model_rc  = nn.Sequential(\n",
    "            \n",
    "            ## conv layer 1\n",
    "            nn.Conv2d(3, 16, kernel_size=5, stride=1, padding=1  ),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.MaxPool2d(2, 2), \n",
    "            nn.Dropout(0.2),\n",
    "            \n",
    "            ## conv layer 2\n",
    "            nn.Conv2d(16, 32, kernel_size=5, stride=1, padding=1  ),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.MaxPool2d(2, 2), \n",
    "            nn.Dropout(0.2),\n",
    "            \n",
    "            ## conv layer 3\n",
    "            nn.Conv2d(32, 32, kernel_size=5, stride=1, padding=1  ),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.MaxPool2d(2, 2), \n",
    "            nn.Dropout(0.2),\n",
    "              \n",
    "            ## FeedForward\n",
    "            \n",
    "            nn.Flatten()\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 128])"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "my_tensor_test   = torch.randn(N_batches_rc, 3, 32,  32)\n",
    "\n",
    "res_actual_model = model_rc(  my_tensor_test   )\n",
    "\n",
    "res_actual_model.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7 (Anaconda 2020.02)",
   "language": "python",
   "name": "anaconda-2020.02-py37"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
